{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedLineDocument\n",
    "import numpy as np\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "t_docs = TaggedLineDocument('data1.txt')\n",
    "words = [t.words for t in t_docs]\n",
    "\n",
    "dic = Dictionary(words)\n",
    "\n",
    "words_maxlen = np.max([len(ws) for ws in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Flatten, Dropout, GRU, Activation\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.core import RepeatVector\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "\n",
    "def discriminator(input_shape, n_units):\n",
    "    model = Sequential()\n",
    "\n",
    "    lrelu = LeakyReLU()\n",
    "   \n",
    "    model.add(GRU(n_units[0], input_shape = input_shape))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(n_units[1], activation = lrelu))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    input = Input(shape = input_shape)\n",
    "    \n",
    "    return Model(input, model(input))\n",
    "\n",
    "def generator(input_shape, output_shape, n_units):\n",
    "    model = Sequential()\n",
    "\n",
    "    lrelu = LeakyReLU()\n",
    "    \n",
    "    model.add(GRU(n_units[0], input_shape = input_shape))\n",
    "    \n",
    "    model.add(RepeatVector(output_shape[0]))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(GRU(n_units[1], return_sequences = True))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(TimeDistributed(Dense(output_shape[1], activation = lrelu)))\n",
    "    model.add(Activation('softmax'))\n",
    "   \n",
    "    model.summary()\n",
    "\n",
    "    input = Input(shape = input_shape)\n",
    "    \n",
    "    return Model(input, model(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "q_shape = (1, 1000)\n",
    "a_shape = (words_maxlen, len(dic))\n",
    "\n",
    "dis_opt = Adam(lr = 1e-5, beta_1 = 0.1)\n",
    "\n",
    "dis = discriminator(a_shape, [256, 128])\n",
    "dis.compile(loss = 'binary_crossentropy', optimizer = dis_opt, metrics = ['acc'])\n",
    "\n",
    "dis.trainable = False\n",
    "\n",
    "gen = generator(q_shape, a_shape, [256, 512])\n",
    "\n",
    "x = Input(shape = q_shape)\n",
    "y = dis(gen(x))\n",
    "\n",
    "m_opt = Adam(lr = 2e-4, beta_1 = 0.5)\n",
    "\n",
    "model = Model(x, y)\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = m_opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_one_hot = lambda ws, size: np.vstack((\n",
    "    np.eye(len(dic))[dic.doc2idx(ws)],\n",
    "    np.zeros((size - len(ws), len(dic)))\n",
    "))\n",
    "\n",
    "input_noise = lambda bsize: np.random.normal(0, 1, (bsize,) + q_shape)\n",
    "\n",
    "def train(epochs, batch_size):\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "    \n",
    "    for ep in range(epochs):\n",
    "        idx = np.random.randint(0, len(words), batch_size)\n",
    "        data = np.array([padding_one_hot(ws, words_maxlen) for ws in np.array(words)[idx]])\n",
    "\n",
    "        gen_data = gen.predict(input_noise(batch_size))\n",
    "\n",
    "        dis_loss_valid = dis.train_on_batch(data, valid)\n",
    "        dis_loss_fake = dis.train_on_batch(gen_data, fake)\n",
    "\n",
    "        dis_loss, dis_acc = 0.5 * np.add(dis_loss_valid, dis_loss_fake)\n",
    "\n",
    "        model_loss = model.train_on_batch(input_noise(batch_size), valid)\n",
    "        \n",
    "        print(f'epoch = {ep}, model loss = {model_loss}, dis loss = {dis_loss}, dis acc = {dis_acc}, dis fake acc = {dis_loss_fake[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer():\n",
    "    y = gen.predict(input_noise(1))\n",
    "\n",
    "    ws = [dic[np.argmax(w)] for w in y[0]]\n",
    "    \n",
    "    return ''.join(ws)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    print( answer() )\n",
    "    print('-----')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
