{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "df = pd.read_csv('data2.txt', delimiter = '\\t', names = ('keyword', 'sentence'))\n",
    "\n",
    "keywords = [k.split(' ') for k in df['keyword'].values]\n",
    "sentences = [s.split(' ') for s in df['sentence'].values]\n",
    "\n",
    "dic = Dictionary(keywords + sentences)\n",
    "\n",
    "q_maxlen = np.max([len(q) for q in keywords])\n",
    "a_maxlen = np.max([len(a) for a in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Flatten, Dropout, GRU, Activation, concatenate\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.core import RepeatVector\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "\n",
    "def discriminator(input_shape1, input_shape2, n_units):\n",
    "    model = Sequential()\n",
    "\n",
    "    lrelu = LeakyReLU()\n",
    "    \n",
    "    input1 = Input(shape = input_shape1)\n",
    "    input2 = Input(shape = input_shape2)\n",
    "\n",
    "    x = concatenate([input1, input2], axis = 1)\n",
    "\n",
    "    x = GRU(n_units[0])(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(n_units[1], activation = lrelu)(x)\n",
    "\n",
    "    output = Dense(1, activation = 'sigmoid')(x)\n",
    "    \n",
    "    model = Model([input1, input2], output)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def generator(input_shape, output_shape, n_units):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(GRU(n_units[0], input_shape = input_shape))\n",
    "\n",
    "    model.add(RepeatVector(output_shape[0]))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU(n_units[1], return_sequences = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(TimeDistributed(Dense(output_shape[1], activation = 'softmax')))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    input = Input(shape = input_shape)\n",
    "\n",
    "    return Model(input, model(input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "q_shape = (q_maxlen, len(dic))\n",
    "a_shape = (a_maxlen, len(dic))\n",
    "\n",
    "dis_opt = Adam(lr = 1e-5, beta_1 = 0.1)\n",
    "\n",
    "dis = discriminator(q_shape, a_shape, [256, 128])\n",
    "dis.compile(loss = 'binary_crossentropy', optimizer = dis_opt, metrics = ['acc'])\n",
    "\n",
    "dis.trainable = False\n",
    "\n",
    "gen = generator(q_shape, a_shape, [256, 512])\n",
    "gen.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['acc'])\n",
    "\n",
    "x = Input(shape = q_shape)\n",
    "y = dis([x, gen(x)])\n",
    "\n",
    "m_opt = Adam(lr = 2e-4, beta_1 = 0.5)\n",
    "\n",
    "model = Model(x, y)\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = m_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_one_hot = lambda ws, size: np.vstack((\n",
    "    np.eye(len(dic))[dic.doc2idx(ws)],\n",
    "    np.zeros((size - len(ws), len(dic)))\n",
    "))\n",
    "\n",
    "def random_data(batch_size):\n",
    "    idx = np.random.randint(0, len(keywords), batch_size)\n",
    "    return get_data(idx)\n",
    "\n",
    "def get_data(idx):\n",
    "    q = np.array([padding_one_hot(ws, q_maxlen) for ws in np.array(keywords)[idx]])\n",
    "    a = np.array([padding_one_hot(ss, a_maxlen) for ss in np.array(sentences)[idx]])\n",
    "    \n",
    "    return (q, a)\n",
    "\n",
    "def train(epochs, batch_size, gen_steps = 1, dis_steps = 1, con_steps = 1):\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "    \n",
    "    for ep in range(epochs):\n",
    "        for i in range(gen_steps):\n",
    "            perm = np.random.permutation(len(keywords))\n",
    "\n",
    "            losses = []\n",
    "            \n",
    "            for j in range(0, len(keywords), batch_size):\n",
    "                q, a = get_data(perm[i:i + batch_size])\n",
    "                losses.append( gen.train_on_batch(q, a) )\n",
    "\n",
    "            loss, acc = np.mean(losses, axis = 0)\n",
    "            \n",
    "            print(f'epoch = {ep}, step = gen-{i}, loss = {loss}, acc = {acc}')\n",
    "            \n",
    "        for i in range(dis_steps):\n",
    "            q, a = random_data(batch_size)\n",
    "\n",
    "            gen_a = gen.predict(q)\n",
    "        \n",
    "            loss_valid = dis.train_on_batch([q, a], valid)\n",
    "            loss_fake = dis.train_on_batch([q, gen_a], fake)\n",
    "\n",
    "            loss, acc = 0.5 * np.add(loss_valid, loss_fake)\n",
    "\n",
    "            print(f'epoch = {ep}, step = dis-{i}, loss = {loss}, acc = {acc}, fake acc = {loss_fake[1]}')\n",
    "            \n",
    "        for i in range(con_steps):\n",
    "            q, _ = random_data(batch_size)\n",
    "\n",
    "            loss = model.train_on_batch(q, valid)\n",
    "\n",
    "            print(f'epoch = {ep}, step = con-{i}, loss = {loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(30, 100, 10)\n",
    "train(100, 100, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(kw):\n",
    "    x = np.array([padding_one_hot(kw, q_maxlen)])\n",
    "\n",
    "    y = gen.predict(x)\n",
    "\n",
    "    res = [dic[np.argmax(w)] for w in y[0]]\n",
    "    \n",
    "    return ''.join(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    q = ''.join(keywords[i])\n",
    "    a = ''.join(sentences[i])\n",
    "\n",
    "    r = answer(keywords[i])\n",
    "    \n",
    "    print(f'q = {q}, a = {a}, result = {r}' )\n",
    "    print('-----')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
