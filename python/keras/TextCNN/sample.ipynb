{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/data.tsv', delimiter = '\\t', names = ('label', 'sentence'))\n",
    "\n",
    "words = [ws.split(' ') for ws in df['sentence']]\n",
    "labels = [v for v in df['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "SPACE = ' '\n",
    "\n",
    "dic = Dictionary(words + [[SPACE]])\n",
    "\n",
    "word_maxlen = np.max([len(w) for w in words])\n",
    "\n",
    "word_maxlen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Conv2D, concatenate\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "def textcnn(sentence_size, embed_size):\n",
    "    input = Input(shape = (sentence_size,))\n",
    "    \n",
    "    x = Embedding(input_dim = len(dic), output_dim = embed_size, input_length = sentence_size)(input)\n",
    "    x = Reshape((sentence_size, embed_size, 1))(x)\n",
    "    \n",
    "    conv1 = Conv2D(512, kernel_size = (3, embed_size), activation = 'relu')(x)\n",
    "    conv2 = Conv2D(512, kernel_size = (4, embed_size), activation = 'relu')(x)\n",
    "    conv3 = Conv2D(512, kernel_size = (5, embed_size), activation = 'relu')(x)\n",
    "    \n",
    "    pool1 = MaxPooling2D(pool_size = (sentence_size - 3 + 1, 1))(conv1)\n",
    "    pool2 = MaxPooling2D(pool_size = (sentence_size - 4 + 1, 1))(conv2)\n",
    "    pool3 = MaxPooling2D(pool_size = (sentence_size - 5 + 1, 1))(conv3)\n",
    "\n",
    "    x = concatenate([pool1, pool2, pool3], axis = 1)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    output = Dense(units = 1, activation = 'sigmoid')(x)\n",
    "    \n",
    "    model = Model(input, output)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = textcnn(word_maxlen, 256)\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 10\n",
    "batch = 50\n",
    "\n",
    "padding_words = lambda d, size: dic.doc2idx(d) + (dic.doc2idx([SPACE]) * (size - len(d)))\n",
    "\n",
    "x = np.array([padding_words(ws, word_maxlen) for ws in words])\n",
    "\n",
    "hist = model.fit(x, labels, epochs = epoch, batch_size = batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print( model.predict(np.array([padding_words(words[i], word_maxlen)])) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
